{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baicheto/AML_Bitcoin/blob/Kri/AML_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE"
      ],
      "metadata": {
        "id": "gpEE5Q6HwRH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the PyG Data object on G_int_sub"
      ],
      "metadata": {
        "id": "JJ7oxRHvxkkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyg_data = from_networkx(G_int_sub)\n",
        "pyg_data.x = feat_intr.clone()\n",
        "\n",
        "X_sub_gs = pyg_data.x.cpu().numpy()"
      ],
      "metadata": {
        "id": "ZLzxuXnTQtLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_nodes_gs = y.size(0)\n",
        "train_mask_gs = torch.zeros(num_nodes_gs, dtype=torch.bool, device=device)\n",
        "val_mask_gs   = torch.zeros(num_nodes_gs, dtype=torch.bool, device=device)\n",
        "test_mask_gs  = torch.zeros(num_nodes_gs, dtype=torch.bool, device=device)\n",
        "\n",
        "mask_known_gs = (y >= 0).to(device)\n",
        "\n",
        "ts_gs = ts_tensor.clone()\n",
        "\n",
        "train_mask_gs[mask_known_gs & (ts_gs <= 30)]                 = True\n",
        "val_mask_gs[  mask_known_gs & ((ts_gs >= 31) & (ts_gs <= 40)) ] = True\n",
        "test_mask_gs[ mask_known_gs & (ts_gs >= 41) ]                 = True"
      ],
      "metadata": {
        "id": "Z94Wmb4_QtLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyg_data.train_mask = train_mask_gs\n",
        "pyg_data.val_mask   = val_mask_gs\n",
        "pyg_data.test_mask  = test_mask_gs"
      ],
      "metadata": {
        "id": "vVxD2WMZQtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_sub_gs = pyg_data.x.cpu().numpy()\n",
        "train_mask_np = pyg_data.train_mask.cpu().numpy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_sub_gs[train_mask_np])\n",
        "X_scaled = scaler.transform(X_sub_gs)"
      ],
      "metadata": {
        "id": "aZbycNCwQtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyg_data.x = torch.tensor(X_scaled, dtype=torch.float, device=device)\n",
        "pyg_data.y = y.clone().to(device)\n",
        "\n",
        "pyg_data.train_mask = pyg_data.train_mask.to(device)\n",
        "pyg_data.val_mask   = pyg_data.val_mask.to(device)\n",
        "pyg_data.test_mask  = pyg_data.test_mask.to(device)\n",
        "\n",
        "pyg_data = pyg_data.to(device)"
      ],
      "metadata": {
        "id": "48xwLbsxQtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GraphSAGE Model"
      ],
      "metadata": {
        "id": "jSyfh52HwRH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerSAGE(nn.Module):\n",
        "    def __init__(self, in_ch, hid_ch, out_ch, dropout=0.35):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_ch, hid_ch, aggr='mean')\n",
        "        self.conv2 = SAGEConv(hid_ch, hid_ch, aggr='mean')\n",
        "        self.lin   = nn.Linear(hid_ch, out_ch)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu( self.conv1(x, edge_index) )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu( self.conv2(x, edge_index) )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.lin(x)"
      ],
      "metadata": {
        "id": "cidxtIRywCy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoLayerSAGE(\n",
        "    in_ch    = pyg_data.x.size(1),\n",
        "    hid_ch  = 48,\n",
        "    out_ch    = 2,\n",
        "    dropout        = 0.35\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "oibQmfiGwc41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders"
      ],
      "metadata": {
        "id": "vLcVo4dcwRH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = NeighborLoader(\n",
        "    pyg_data,\n",
        "    input_nodes   = pyg_data.train_mask,\n",
        "    num_neighbors = [2],\n",
        "    batch_size    = 512,\n",
        "    shuffle       = True\n",
        ")\n",
        "val_loader = NeighborLoader(\n",
        "    pyg_data,\n",
        "    input_nodes   = pyg_data.val_mask,\n",
        "    num_neighbors = [2],\n",
        "    batch_size    = 512,\n",
        "    shuffle       = False\n",
        ")\n",
        "test_loader = NeighborLoader(\n",
        "    pyg_data,\n",
        "    input_nodes   = pyg_data.test_mask,\n",
        "    num_neighbors = [2],\n",
        "    batch_size    = 512,\n",
        "    shuffle       = False\n",
        ")"
      ],
      "metadata": {
        "id": "Pq4gV1b-wRH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup"
      ],
      "metadata": {
        "id": "0yKbREKzwRH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr          = 0.0248,\n",
        "    weight_decay= 1e-4\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "kJMcV6FEwRH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        logits = out[:batch.batch_size]\n",
        "        labels_batch = batch.y[:batch.batch_size]\n",
        "        loss = criterion(logits, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch.batch_size\n",
        "        total_samples += batch.batch_size\n",
        "    return total_loss / total_samples"
      ],
      "metadata": {
        "id": "mqBCgJrkwRH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_split(loader):\n",
        "    model.eval()\n",
        "    all_true = []\n",
        "    all_probs = []\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        logits = out[:batch.batch_size]\n",
        "        probs = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "        y_true = batch.y[:batch.batch_size].cpu().numpy()\n",
        "        all_true.append(y_true)\n",
        "        all_probs.append(probs)\n",
        "    y_true = np.concatenate(all_true, axis=0)\n",
        "    y_prob = np.concatenate(all_probs, axis=0)\n",
        "    return y_true, y_prob"
      ],
      "metadata": {
        "id": "XUF_CTLMwRH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs    = 498\n",
        "patience      = 80\n",
        "best_val_pr   = -1.0\n",
        "best_state    = None\n",
        "patience_ctr  = 0\n",
        "\n",
        "val_pr_history = []\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    loss = train_one_epoch()\n",
        "\n",
        "    y_val_true, y_val_prob = evaluate_split(val_loader)\n",
        "    val_pr = average_precision_score(y_val_true, y_val_prob)\n",
        "    val_roc = roc_auc_score(y_val_true, y_val_prob)\n",
        "\n",
        "    val_pr_history.append(val_pr)\n",
        "    print(f\"Epoch {epoch:03d}  Loss: {loss:.4f}  Val PR: {val_pr:.4f}  Val ROC: {val_roc:.4f}\")\n",
        "\n",
        "    if val_pr > best_val_pr:\n",
        "        best_val_pr = val_pr\n",
        "        patience_ctr = 0\n",
        "        torch.save(model.state_dict(), \"sage_best.pt\")\n",
        "    else:\n",
        "        patience_ctr += 1\n",
        "        if patience_ctr >= patience:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch}.\")\n",
        "            print(f\"Best Val PR was {best_val_pr:.4f}.\\n\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(\"sage_best.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjl-D4U55Fdj",
        "outputId": "81d1b9ac-5d72-4d5e-d2e3-e596fe535512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001  Loss: 0.2581  Val PR: 0.6938  Val ROC: 0.9164\n",
            "Epoch 002  Loss: 0.2046  Val PR: 0.6698  Val ROC: 0.9258\n",
            "Epoch 003  Loss: 0.1870  Val PR: 0.7868  Val ROC: 0.9417\n",
            "Epoch 004  Loss: 0.1895  Val PR: 0.7858  Val ROC: 0.9370\n",
            "Epoch 005  Loss: 0.1680  Val PR: 0.8101  Val ROC: 0.9507\n",
            "Epoch 006  Loss: 0.1624  Val PR: 0.8038  Val ROC: 0.9456\n",
            "Epoch 007  Loss: 0.1645  Val PR: 0.7300  Val ROC: 0.9378\n",
            "Epoch 008  Loss: 0.1808  Val PR: 0.7186  Val ROC: 0.9293\n",
            "Epoch 009  Loss: 0.1972  Val PR: 0.6938  Val ROC: 0.9309\n",
            "Epoch 010  Loss: 0.1698  Val PR: 0.7829  Val ROC: 0.9423\n",
            "Epoch 011  Loss: 0.1825  Val PR: 0.7569  Val ROC: 0.9377\n",
            "Epoch 012  Loss: 0.1524  Val PR: 0.8296  Val ROC: 0.9472\n",
            "Epoch 013  Loss: 0.1555  Val PR: 0.7893  Val ROC: 0.9441\n",
            "Epoch 014  Loss: 0.1830  Val PR: 0.7623  Val ROC: 0.9291\n",
            "Epoch 015  Loss: 0.2020  Val PR: 0.7265  Val ROC: 0.9295\n",
            "Epoch 016  Loss: 0.1849  Val PR: 0.7712  Val ROC: 0.9319\n",
            "Epoch 017  Loss: 0.1683  Val PR: 0.7240  Val ROC: 0.9334\n",
            "Epoch 018  Loss: 0.1701  Val PR: 0.6277  Val ROC: 0.9285\n",
            "Epoch 019  Loss: 0.1624  Val PR: 0.7372  Val ROC: 0.9429\n",
            "Epoch 020  Loss: 0.1637  Val PR: 0.7973  Val ROC: 0.9420\n",
            "Epoch 021  Loss: 0.1612  Val PR: 0.7661  Val ROC: 0.9392\n",
            "Epoch 022  Loss: 0.1662  Val PR: 0.8028  Val ROC: 0.9455\n",
            "Epoch 023  Loss: 0.1614  Val PR: 0.7676  Val ROC: 0.9427\n",
            "Epoch 024  Loss: 0.1569  Val PR: 0.7748  Val ROC: 0.9297\n",
            "Epoch 025  Loss: 0.1654  Val PR: 0.7279  Val ROC: 0.9378\n",
            "Epoch 026  Loss: 0.1600  Val PR: 0.7678  Val ROC: 0.9416\n",
            "Epoch 027  Loss: 0.1525  Val PR: 0.7362  Val ROC: 0.9419\n",
            "Epoch 028  Loss: 0.1461  Val PR: 0.7813  Val ROC: 0.9426\n",
            "Epoch 029  Loss: 0.1657  Val PR: 0.6869  Val ROC: 0.9237\n",
            "Epoch 030  Loss: 0.1669  Val PR: 0.7805  Val ROC: 0.9447\n",
            "Epoch 031  Loss: 0.1540  Val PR: 0.7423  Val ROC: 0.9425\n",
            "Epoch 032  Loss: 0.1844  Val PR: 0.7553  Val ROC: 0.9372\n",
            "Epoch 033  Loss: 0.1687  Val PR: 0.7864  Val ROC: 0.9433\n",
            "Epoch 034  Loss: 0.1693  Val PR: 0.7904  Val ROC: 0.9407\n",
            "Epoch 035  Loss: 0.1623  Val PR: 0.8114  Val ROC: 0.9467\n",
            "Epoch 036  Loss: 0.1543  Val PR: 0.7890  Val ROC: 0.9445\n",
            "Epoch 037  Loss: 0.1519  Val PR: 0.8269  Val ROC: 0.9537\n",
            "Epoch 038  Loss: 0.1480  Val PR: 0.8315  Val ROC: 0.9511\n",
            "Epoch 039  Loss: 0.1520  Val PR: 0.8112  Val ROC: 0.9488\n",
            "Epoch 040  Loss: 0.1522  Val PR: 0.7836  Val ROC: 0.9444\n",
            "Epoch 041  Loss: 0.1639  Val PR: 0.7454  Val ROC: 0.9411\n",
            "Epoch 042  Loss: 0.1550  Val PR: 0.7753  Val ROC: 0.9440\n",
            "Epoch 043  Loss: 0.1677  Val PR: 0.7423  Val ROC: 0.9440\n",
            "Epoch 044  Loss: 0.1987  Val PR: 0.7719  Val ROC: 0.9436\n",
            "Epoch 045  Loss: 0.2360  Val PR: 0.6902  Val ROC: 0.9247\n",
            "Epoch 046  Loss: 0.2040  Val PR: 0.7133  Val ROC: 0.9274\n",
            "Epoch 047  Loss: 0.1993  Val PR: 0.7040  Val ROC: 0.9260\n",
            "Epoch 048  Loss: 0.1795  Val PR: 0.6660  Val ROC: 0.9263\n",
            "Epoch 049  Loss: 0.1783  Val PR: 0.8017  Val ROC: 0.9455\n",
            "Epoch 050  Loss: 0.1701  Val PR: 0.7910  Val ROC: 0.9426\n",
            "Epoch 051  Loss: 0.1648  Val PR: 0.7805  Val ROC: 0.9430\n",
            "Epoch 052  Loss: 0.1702  Val PR: 0.7541  Val ROC: 0.9404\n",
            "Epoch 053  Loss: 0.1798  Val PR: 0.7386  Val ROC: 0.9356\n",
            "Epoch 054  Loss: 0.1718  Val PR: 0.7426  Val ROC: 0.9349\n",
            "Epoch 055  Loss: 0.1661  Val PR: 0.7882  Val ROC: 0.9480\n",
            "Epoch 056  Loss: 0.1572  Val PR: 0.7949  Val ROC: 0.9448\n",
            "Epoch 057  Loss: 0.1557  Val PR: 0.7984  Val ROC: 0.9487\n",
            "Epoch 058  Loss: 0.1599  Val PR: 0.7743  Val ROC: 0.9428\n",
            "Epoch 059  Loss: 0.1580  Val PR: 0.7711  Val ROC: 0.9465\n",
            "Epoch 060  Loss: 0.1639  Val PR: 0.7763  Val ROC: 0.9439\n",
            "Epoch 061  Loss: 0.1525  Val PR: 0.7674  Val ROC: 0.9469\n",
            "Epoch 062  Loss: 0.1484  Val PR: 0.8253  Val ROC: 0.9493\n",
            "Epoch 063  Loss: 0.1520  Val PR: 0.7309  Val ROC: 0.9402\n",
            "Epoch 064  Loss: 0.1544  Val PR: 0.8106  Val ROC: 0.9442\n",
            "Epoch 065  Loss: 0.1558  Val PR: 0.7782  Val ROC: 0.9443\n",
            "Epoch 066  Loss: 0.1560  Val PR: 0.7436  Val ROC: 0.9417\n",
            "Epoch 067  Loss: 0.1529  Val PR: 0.7962  Val ROC: 0.9443\n",
            "Epoch 068  Loss: 0.1514  Val PR: 0.8318  Val ROC: 0.9534\n",
            "Epoch 069  Loss: 0.1480  Val PR: 0.7678  Val ROC: 0.9439\n",
            "Epoch 070  Loss: 0.1480  Val PR: 0.7924  Val ROC: 0.9503\n",
            "Epoch 071  Loss: 0.1493  Val PR: 0.7793  Val ROC: 0.9473\n",
            "Epoch 072  Loss: 0.1496  Val PR: 0.7436  Val ROC: 0.9420\n",
            "Epoch 073  Loss: 0.1544  Val PR: 0.8112  Val ROC: 0.9452\n",
            "Epoch 074  Loss: 0.1495  Val PR: 0.8030  Val ROC: 0.9482\n",
            "Epoch 075  Loss: 0.1556  Val PR: 0.8077  Val ROC: 0.9467\n",
            "Epoch 076  Loss: 0.1465  Val PR: 0.8203  Val ROC: 0.9511\n",
            "Epoch 077  Loss: 0.1562  Val PR: 0.7492  Val ROC: 0.9296\n",
            "Epoch 078  Loss: 0.1719  Val PR: 0.7400  Val ROC: 0.9240\n",
            "Epoch 079  Loss: 0.1711  Val PR: 0.7753  Val ROC: 0.9414\n",
            "Epoch 080  Loss: 0.1703  Val PR: 0.7555  Val ROC: 0.9288\n",
            "Epoch 081  Loss: 0.1592  Val PR: 0.7654  Val ROC: 0.9388\n",
            "Epoch 082  Loss: 0.1519  Val PR: 0.7775  Val ROC: 0.9414\n",
            "Epoch 083  Loss: 0.1542  Val PR: 0.8234  Val ROC: 0.9509\n",
            "Epoch 084  Loss: 0.1476  Val PR: 0.7839  Val ROC: 0.9376\n",
            "Epoch 085  Loss: 0.1566  Val PR: 0.7722  Val ROC: 0.9436\n",
            "Epoch 086  Loss: 0.1547  Val PR: 0.7812  Val ROC: 0.9442\n",
            "Epoch 087  Loss: 0.1545  Val PR: 0.6911  Val ROC: 0.9401\n",
            "Epoch 088  Loss: 0.1660  Val PR: 0.7971  Val ROC: 0.9457\n",
            "Epoch 089  Loss: 0.1684  Val PR: 0.7880  Val ROC: 0.9421\n",
            "Epoch 090  Loss: 0.1692  Val PR: 0.7044  Val ROC: 0.9353\n",
            "Epoch 091  Loss: 0.1614  Val PR: 0.7841  Val ROC: 0.9404\n",
            "Epoch 092  Loss: 0.1591  Val PR: 0.8130  Val ROC: 0.9421\n",
            "Epoch 093  Loss: 0.1574  Val PR: 0.8166  Val ROC: 0.9543\n",
            "Epoch 094  Loss: 0.1489  Val PR: 0.7822  Val ROC: 0.9435\n",
            "Epoch 095  Loss: 0.1588  Val PR: 0.7650  Val ROC: 0.9406\n",
            "Epoch 096  Loss: 0.1658  Val PR: 0.7755  Val ROC: 0.9328\n",
            "Epoch 097  Loss: 0.1823  Val PR: 0.7551  Val ROC: 0.9358\n",
            "Epoch 098  Loss: 0.1611  Val PR: 0.8006  Val ROC: 0.9379\n",
            "Epoch 099  Loss: 0.1820  Val PR: 0.7150  Val ROC: 0.9274\n",
            "Epoch 100  Loss: 0.1623  Val PR: 0.7966  Val ROC: 0.9443\n",
            "Epoch 101  Loss: 0.1631  Val PR: 0.6069  Val ROC: 0.9190\n",
            "Epoch 102  Loss: 0.1613  Val PR: 0.7746  Val ROC: 0.9421\n",
            "Epoch 103  Loss: 0.1504  Val PR: 0.8224  Val ROC: 0.9489\n",
            "Epoch 104  Loss: 0.1427  Val PR: 0.7597  Val ROC: 0.9395\n",
            "Epoch 105  Loss: 0.1525  Val PR: 0.7436  Val ROC: 0.9306\n",
            "Epoch 106  Loss: 0.1617  Val PR: 0.8089  Val ROC: 0.9474\n",
            "Epoch 107  Loss: 0.1521  Val PR: 0.8197  Val ROC: 0.9553\n",
            "Epoch 108  Loss: 0.1562  Val PR: 0.7711  Val ROC: 0.9430\n",
            "Epoch 109  Loss: 0.1488  Val PR: 0.8226  Val ROC: 0.9528\n",
            "Epoch 110  Loss: 0.1473  Val PR: 0.7894  Val ROC: 0.9449\n",
            "Epoch 111  Loss: 0.1448  Val PR: 0.7997  Val ROC: 0.9475\n",
            "Epoch 112  Loss: 0.1557  Val PR: 0.7893  Val ROC: 0.9436\n",
            "Epoch 113  Loss: 0.1594  Val PR: 0.7892  Val ROC: 0.9369\n",
            "Epoch 114  Loss: 0.1794  Val PR: 0.7497  Val ROC: 0.9323\n",
            "Epoch 115  Loss: 0.1530  Val PR: 0.7929  Val ROC: 0.9457\n",
            "Epoch 116  Loss: 0.1568  Val PR: 0.7753  Val ROC: 0.9479\n",
            "Epoch 117  Loss: 0.1599  Val PR: 0.7784  Val ROC: 0.9377\n",
            "Epoch 118  Loss: 0.1941  Val PR: 0.7611  Val ROC: 0.9398\n",
            "Epoch 119  Loss: 0.1971  Val PR: 0.7595  Val ROC: 0.9411\n",
            "Epoch 120  Loss: 0.2257  Val PR: 0.7581  Val ROC: 0.9393\n",
            "Epoch 121  Loss: 0.1924  Val PR: 0.7579  Val ROC: 0.9378\n",
            "Epoch 122  Loss: 0.1753  Val PR: 0.7593  Val ROC: 0.9353\n",
            "Epoch 123  Loss: 0.1693  Val PR: 0.7471  Val ROC: 0.9275\n",
            "Epoch 124  Loss: 0.1783  Val PR: 0.7125  Val ROC: 0.9171\n",
            "Epoch 125  Loss: 0.1808  Val PR: 0.7513  Val ROC: 0.9395\n",
            "Epoch 126  Loss: 0.1720  Val PR: 0.7706  Val ROC: 0.9382\n",
            "Epoch 127  Loss: 0.1780  Val PR: 0.7135  Val ROC: 0.9202\n",
            "Epoch 128  Loss: 0.1775  Val PR: 0.7033  Val ROC: 0.9303\n",
            "Epoch 129  Loss: 0.1757  Val PR: 0.7043  Val ROC: 0.9173\n",
            "Epoch 130  Loss: 0.1871  Val PR: 0.7168  Val ROC: 0.9272\n",
            "Epoch 131  Loss: 0.1718  Val PR: 0.7340  Val ROC: 0.9244\n",
            "Epoch 132  Loss: 0.1648  Val PR: 0.7872  Val ROC: 0.9477\n",
            "Epoch 133  Loss: 0.1680  Val PR: 0.6751  Val ROC: 0.9249\n",
            "Epoch 134  Loss: 0.1685  Val PR: 0.6231  Val ROC: 0.9238\n",
            "Epoch 135  Loss: 0.1673  Val PR: 0.7796  Val ROC: 0.9446\n",
            "Epoch 136  Loss: 0.1625  Val PR: 0.7576  Val ROC: 0.9408\n",
            "Epoch 137  Loss: 0.1534  Val PR: 0.6577  Val ROC: 0.9282\n",
            "Epoch 138  Loss: 0.1524  Val PR: 0.7420  Val ROC: 0.9337\n",
            "Epoch 139  Loss: 0.1695  Val PR: 0.7384  Val ROC: 0.9322\n",
            "Epoch 140  Loss: 0.1682  Val PR: 0.7761  Val ROC: 0.9371\n",
            "Epoch 141  Loss: 0.1686  Val PR: 0.6747  Val ROC: 0.9283\n",
            "Epoch 142  Loss: 0.1622  Val PR: 0.7709  Val ROC: 0.9419\n",
            "Epoch 143  Loss: 0.1594  Val PR: 0.7573  Val ROC: 0.9377\n",
            "Epoch 144  Loss: 0.2048  Val PR: 0.7771  Val ROC: 0.9303\n",
            "Epoch 145  Loss: 0.2157  Val PR: 0.7470  Val ROC: 0.9363\n",
            "Epoch 146  Loss: 0.1900  Val PR: 0.7145  Val ROC: 0.9102\n",
            "Epoch 147  Loss: 0.1821  Val PR: 0.7305  Val ROC: 0.9267\n",
            "Epoch 148  Loss: 0.1683  Val PR: 0.7662  Val ROC: 0.9410\n",
            "\n",
            "Early stopping triggered at epoch 148.\n",
            "Best Val PR was 0.8318.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Test Evaluation"
      ],
      "metadata": {
        "id": "yAoCM7HfwRIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out_all = model(pyg_data.x, pyg_data.edge_index)\n",
        "    probs_all = torch.softmax(out_all, dim=1)[:,1].cpu().numpy()\n",
        "    labels_all = pyg_data.y.cpu().numpy()"
      ],
      "metadata": {
        "id": "O5uNsV2KwRIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mask_np = pyg_data.test_mask.cpu().numpy()\n",
        "y_test_true  = labels_all[test_mask_np]\n",
        "y_test_prob  = probs_all[test_mask_np]\n",
        "\n",
        "M_test = y_test_true.shape[0]\n",
        "train_mask_np = pyg_data.train_mask.cpu().numpy()\n",
        "y_train_true  = labels_all[train_mask_np]\n",
        "prevalence    = y_train_true.mean()"
      ],
      "metadata": {
        "id": "H1kGzgLdwRIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutoffs = {\n",
        "    \"Top 0.1%\":        max(1, int(0.001 * M_test)),\n",
        "    \"Top 1%\":          max(1, int(0.01  * M_test)),\n",
        "    \"Top 10%\":         max(1, int(0.10  * M_test)),\n",
        "    \"Prevalence\": max(1, int(prevalence * M_test))\n",
        "}"
      ],
      "metadata": {
        "id": "CzwzGQKawRIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_runs = 100\n",
        "metrics = {\n",
        "    \"roc_auc\":     [],\n",
        "    \"pr_auc\":      [],\n",
        "    **{f\"{name}_P\": [] for name in cutoffs},\n",
        "    **{f\"{name}_R\": [] for name in cutoffs},\n",
        "    **{f\"{name}_F1\": [] for name in cutoffs},\n",
        "}\n",
        "\n",
        "rng       = np.random.RandomState(42)\n",
        "half_size = M_test // 2"
      ],
      "metadata": {
        "id": "HLH8T_iBwRIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(n_runs):\n",
        "    idxs = rng.choice(M_test, size=half_size, replace=False)\n",
        "    y_true_bs = y_test_true[idxs]\n",
        "    y_prob_bs = y_test_prob[idxs]\n",
        "\n",
        "    metrics[\"roc_auc\"].append(roc_auc_score(y_true_bs, y_prob_bs))\n",
        "    metrics[\"pr_auc\"].append(average_precision_score(y_true_bs, y_prob_bs))\n",
        "\n",
        "    sorted_idx = np.argsort(y_prob_bs)\n",
        "    for name, k in cutoffs.items():\n",
        "        topk = sorted_idx[-k:]\n",
        "        y_pred_bs = np.zeros_like(y_prob_bs, dtype=int)\n",
        "        y_pred_bs[topk] = 1\n",
        "\n",
        "        metrics[f\"{name}_P\"].append(precision_score(y_true_bs, y_pred_bs, zero_division=0))\n",
        "        metrics[f\"{name}_R\"].append(recall_score(y_true_bs, y_pred_bs, zero_division=0))\n",
        "        metrics[f\"{name}_F1\"].append(f1_score(y_true_bs, y_pred_bs, zero_division=0))"
      ],
      "metadata": {
        "id": "7IZNtAiewRIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fmt(name):\n",
        "    arr = np.array(metrics[name])\n",
        "    return f\"{arr.mean():.3f} ± {arr.std():.3f}\"\n",
        "\n",
        "print(\"\\n=== GraphSAGE Test Results (n=100 bootstraps) ===\")\n",
        "print(f\"AUC-ROC  : {fmt('roc_auc')}\")\n",
        "print(f\"AUC-PR   : {fmt('pr_auc')}\")\n",
        "for name in cutoffs:\n",
        "    print(f\"{name:12} Precision: {fmt(name + '_P')}\")\n",
        "    print(f\"{name:12} Recall   : {fmt(name + '_R')}\")\n",
        "    print(f\"{name:12} F1-score : {fmt(name + '_F1')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qOs_Yxn5CYz",
        "outputId": "edb51e21-f9d1-4cff-9b94-d9c1e84dbd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GraphSAGE Test Results (n=100 bootstraps) ===\n",
            "AUC-ROC  : 0.807 ± 0.011\n",
            "AUC-PR   : 0.511 ± 0.025\n",
            "Top 0.1%     Precision: 0.506 ± 0.269\n",
            "Top 0.1%     Recall   : 0.014 ± 0.010\n",
            "Top 0.1%     F1-score : 0.028 ± 0.020\n",
            "Top 1%       Precision: 0.938 ± 0.029\n",
            "Top 1%       Recall   : 0.267 ± 0.096\n",
            "Top 1%       F1-score : 0.408 ± 0.119\n",
            "Top 10%      Precision: 0.234 ± 0.068\n",
            "Top 10%      Recall   : 0.604 ± 0.041\n",
            "Top 10%      F1-score : 0.329 ± 0.066\n",
            "Prevalence   Precision: 0.217 ± 0.063\n",
            "Prevalence   Recall   : 0.616 ± 0.041\n",
            "Prevalence   F1-score : 0.314 ± 0.065\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0yKbREKzwRH_",
        "yAoCM7HfwRIA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}