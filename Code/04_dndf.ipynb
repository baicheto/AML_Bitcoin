{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baicheto/AML_Bitcoin/blob/Kri/AML_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ziCC44A_Am"
      },
      "source": [
        "# Deep Neural Decision Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Deep Neural Decision Forest` takes the same input as `DeepWalk` which is `feat_tensor`."
      ],
      "metadata": {
        "id": "d3yOnvpUeM9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DNDF Machinery"
      ],
      "metadata": {
        "id": "kUi7PTHcI2fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralDecisionForest(nn.Module):\n",
        "    def __init__(self, input_dim, num_trees, depth, num_classes):\n",
        "        super().__init__()\n",
        "        self.num_trees    = num_trees\n",
        "        self.depth        = depth\n",
        "        self.num_leaves   = 2 ** depth\n",
        "        self.num_dec_nodes= self.num_leaves - 1\n",
        "\n",
        "        self.router = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_trees * self.num_dec_nodes)\n",
        "        )\n",
        "\n",
        "        self.leaf_probs = nn.Parameter(\n",
        "            torch.randn(num_trees, self.num_leaves, num_classes) * 0.1\n",
        "        )\n",
        "\n",
        "        mask = torch.zeros(self.num_leaves, self.num_dec_nodes)\n",
        "        for l in range(self.num_leaves):\n",
        "            idx = 0\n",
        "            for d in range(depth):\n",
        "                bit = (l >> (depth-1-d)) & 1\n",
        "                mask[l, idx] = float(1 - bit)\n",
        "                idx = 2*idx + 1 + bit\n",
        "                if idx >= self.num_dec_nodes: break\n",
        "        self.register_buffer('leaf_mask', mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        logits = self.router(x)\n",
        "        probs  = torch.sigmoid(logits).view(B, self.num_trees, self.num_dec_nodes)\n",
        "\n",
        "        p = probs.unsqueeze(-2)\n",
        "        m = self.leaf_mask.unsqueeze(0).unsqueeze(0)\n",
        "        eps = 1e-6\n",
        "        p = p.clamp(eps, 1-eps)\n",
        "        leaf_reach = (p**m * (1-p)**(1-m)).prod(dim=-1)\n",
        "\n",
        "        leaf_dist = torch.softmax(self.leaf_probs, dim=-1).unsqueeze(0)\n",
        "        out = (leaf_reach.unsqueeze(-1) * leaf_dist).sum(dim=[1,2])\n",
        "        return out"
      ],
      "metadata": {
        "id": "Uofm1qvyShQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnf = NeuralDecisionForest(\n",
        "    input_dim   = feat_tensor.size(1),\n",
        "    num_trees   = 10,\n",
        "    depth       = 4,\n",
        "    num_classes = 2\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "xmtyJSkSS2IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup"
      ],
      "metadata": {
        "id": "zZ1tlEipJQ1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(dnf.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,\n",
        "            mode='max', factor=0.5, patience=5, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "miQq5q3Ce2dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val, wait = -1, 0\n",
        "patience = 15\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    dnf.train()\n",
        "    idxs = train_idx.cpu().numpy()\n",
        "    np.random.shuffle(idxs)\n",
        "    losses = []\n",
        "    for b in range(0, len(idxs), 1024):\n",
        "        batch = torch.tensor(idxs[b:b+1024], device=device)\n",
        "        logits= dnf(feat_tensor[batch])\n",
        "        loss  = criterion(logits, labels[batch])\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    dnf.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = dnf(feat_tensor[val_idx])\n",
        "        val_probs  = F.softmax(val_logits, dim=1)[:,1].cpu().numpy()\n",
        "        val_true   = labels[val_idx].cpu().numpy()\n",
        "    val_pr = average_precision_score(val_true, val_probs)\n",
        "    sched.step(val_pr)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d}  loss={np.mean(losses):.4f}  Val PR={val_pr:.4f}\")\n",
        "    if val_pr > best_val:\n",
        "        best_val, wait = val_pr, 0\n",
        "        torch.save(dnf.state_dict(), \"dnf_best.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "dnf.load_state_dict(torch.load(\"dnf_best.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2cj_yaVSpmR",
        "outputId": "36859c30-917c-4a3a-e222-4095c7748f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001  loss=0.6916  Val PR=0.6931\n",
            "Epoch 002  loss=0.6304  Val PR=0.6527\n",
            "Epoch 003  loss=0.5198  Val PR=0.3922\n",
            "Epoch 004  loss=0.4196  Val PR=0.3108\n",
            "Epoch 005  loss=0.3364  Val PR=0.2693\n",
            "Epoch 006  loss=0.2848  Val PR=0.2944\n",
            "Epoch 007  loss=0.2583  Val PR=0.3605\n",
            "Epoch 008  loss=0.2447  Val PR=0.3881\n",
            "Epoch 009  loss=0.2370  Val PR=0.4454\n",
            "Epoch 010  loss=0.2291  Val PR=0.5305\n",
            "Epoch 011  loss=0.2216  Val PR=0.6476\n",
            "Epoch 012  loss=0.2161  Val PR=0.7231\n",
            "Epoch 013  loss=0.2101  Val PR=0.7512\n",
            "Epoch 014  loss=0.2059  Val PR=0.7796\n",
            "Epoch 015  loss=0.1997  Val PR=0.7918\n",
            "Epoch 016  loss=0.1975  Val PR=0.8031\n",
            "Epoch 017  loss=0.1924  Val PR=0.8109\n",
            "Epoch 018  loss=0.1888  Val PR=0.8126\n",
            "Epoch 019  loss=0.1872  Val PR=0.8153\n",
            "Epoch 020  loss=0.1849  Val PR=0.8184\n",
            "Epoch 021  loss=0.1824  Val PR=0.8192\n",
            "Epoch 022  loss=0.1797  Val PR=0.8207\n",
            "Epoch 023  loss=0.1788  Val PR=0.8225\n",
            "Epoch 024  loss=0.1759  Val PR=0.8243\n",
            "Epoch 025  loss=0.1760  Val PR=0.8248\n",
            "Epoch 026  loss=0.1734  Val PR=0.8262\n",
            "Epoch 027  loss=0.1729  Val PR=0.8281\n",
            "Epoch 028  loss=0.1720  Val PR=0.8292\n",
            "Epoch 029  loss=0.1688  Val PR=0.8319\n",
            "Epoch 030  loss=0.1683  Val PR=0.8333\n",
            "Epoch 031  loss=0.1670  Val PR=0.8368\n",
            "Epoch 032  loss=0.1661  Val PR=0.8382\n",
            "Epoch 033  loss=0.1659  Val PR=0.8400\n",
            "Epoch 034  loss=0.1636  Val PR=0.8415\n",
            "Epoch 035  loss=0.1634  Val PR=0.8423\n",
            "Epoch 036  loss=0.1622  Val PR=0.8441\n",
            "Epoch 037  loss=0.1615  Val PR=0.8441\n",
            "Epoch 038  loss=0.1602  Val PR=0.8461\n",
            "Epoch 039  loss=0.1602  Val PR=0.8463\n",
            "Epoch 040  loss=0.1593  Val PR=0.8467\n",
            "Epoch 041  loss=0.1584  Val PR=0.8482\n",
            "Epoch 042  loss=0.1585  Val PR=0.8482\n",
            "Epoch 043  loss=0.1582  Val PR=0.8482\n",
            "Epoch 044  loss=0.1567  Val PR=0.8494\n",
            "Epoch 045  loss=0.1563  Val PR=0.8505\n",
            "Epoch 046  loss=0.1545  Val PR=0.8508\n",
            "Epoch 047  loss=0.1544  Val PR=0.8507\n",
            "Epoch 048  loss=0.1549  Val PR=0.8509\n",
            "Epoch 049  loss=0.1530  Val PR=0.8512\n",
            "Epoch 050  loss=0.1524  Val PR=0.8517\n",
            "Epoch 051  loss=0.1545  Val PR=0.8524\n",
            "Epoch 052  loss=0.1541  Val PR=0.8534\n",
            "Epoch 053  loss=0.1522  Val PR=0.8536\n",
            "Epoch 054  loss=0.1532  Val PR=0.8534\n",
            "Epoch 055  loss=0.1523  Val PR=0.8559\n",
            "Epoch 056  loss=0.1523  Val PR=0.8580\n",
            "Epoch 057  loss=0.1510  Val PR=0.8603\n",
            "Epoch 058  loss=0.1509  Val PR=0.8616\n",
            "Epoch 059  loss=0.1500  Val PR=0.8682\n",
            "Epoch 060  loss=0.1508  Val PR=0.8777\n",
            "Epoch 061  loss=0.1497  Val PR=0.8904\n",
            "Epoch 062  loss=0.1490  Val PR=0.8996\n",
            "Epoch 063  loss=0.1496  Val PR=0.8993\n",
            "Epoch 064  loss=0.1490  Val PR=0.9005\n",
            "Epoch 065  loss=0.1463  Val PR=0.8967\n",
            "Epoch 066  loss=0.1422  Val PR=0.8946\n",
            "Epoch 067  loss=0.1370  Val PR=0.8920\n",
            "Epoch 068  loss=0.1335  Val PR=0.8891\n",
            "Epoch 069  loss=0.1311  Val PR=0.8878\n",
            "Epoch 070  loss=0.1264  Val PR=0.8871\n",
            "Epoch 071  loss=0.1256  Val PR=0.8866\n",
            "Epoch 072  loss=0.1245  Val PR=0.8872\n",
            "Epoch 073  loss=0.1235  Val PR=0.8868\n",
            "Epoch 074  loss=0.1216  Val PR=0.8872\n",
            "Epoch 075  loss=0.1209  Val PR=0.8869\n",
            "Epoch 076  loss=0.1188  Val PR=0.8868\n",
            "Epoch 077  loss=0.1192  Val PR=0.8872\n",
            "Epoch 078  loss=0.1188  Val PR=0.8872\n",
            "Epoch 079  loss=0.1206  Val PR=0.8871\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Test Evaluation"
      ],
      "metadata": {
        "id": "0Nt-aRFz96kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnf.eval()\n",
        "with torch.no_grad():\n",
        "    logits_all = dnf(feat_tensor)\n",
        "    probs_all  = torch.softmax(logits_all, dim=1)[:,1].cpu().numpy()\n",
        "    labels_all = labels.cpu().numpy()\n",
        "\n",
        "y_test_true = labels_all[test_idx.cpu().numpy()]\n",
        "y_test_prob = probs_all[test_idx.cpu().numpy()]\n",
        "M_test   = y_test_true.shape[0]\n",
        "prevalence = y_test_true.mean()"
      ],
      "metadata": {
        "id": "6RBsqMvYIklX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutoffs = {\n",
        "    \"Top 0.1%\": max(1, int(0.001 * M_test)),\n",
        "    \"Top 1%\":   max(1, int(0.01  * M_test)),\n",
        "    \"Top 10%\":  max(1, int(0.10  * M_test)),\n",
        "    \"Prevalence\": max(1, int(prevalence * M_test)),\n",
        "}"
      ],
      "metadata": {
        "id": "oEFhjZ9cfQg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_runs = 100\n",
        "metrics = {\n",
        "    \"roc_auc\": [],\n",
        "    \"pr_auc\":  [],\n",
        "    **{f\"{name}_P\": [] for name in cutoffs},\n",
        "    **{f\"{name}_R\": [] for name in cutoffs},\n",
        "    **{f\"{name}_F1\": [] for name in cutoffs},\n",
        "}\n",
        "\n",
        "rng       = np.random.RandomState(42)\n",
        "half_size = M_test // 2"
      ],
      "metadata": {
        "id": "OfCAUUzu-I8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for run_i in range(n_runs):\n",
        "    idxs = rng.choice(M_test, size=half_size, replace=False)\n",
        "    y_bs = y_test_true[idxs]\n",
        "    p_bs = y_test_prob[idxs]\n",
        "\n",
        "    metrics[\"roc_auc\"].append(roc_auc_score(y_bs, p_bs))\n",
        "    metrics[\"pr_auc\"].append(average_precision_score(y_bs, p_bs))\n",
        "\n",
        "    order = np.argsort(p_bs)\n",
        "    for name, k in cutoffs.items():\n",
        "        topk = order[-k:]\n",
        "        pred = np.zeros_like(p_bs, dtype=int)\n",
        "        pred[topk] = 1\n",
        "\n",
        "        metrics[f\"{name}_P\"].append(precision_score(y_bs, pred, zero_division=0))\n",
        "        metrics[f\"{name}_R\"].append(recall_score(y_bs, pred, zero_division=0))\n",
        "        metrics[f\"{name}_F1\"].append(f1_score(y_bs, pred, zero_division=0))"
      ],
      "metadata": {
        "id": "iHMtHdXF-Lqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fmt(arr):\n",
        "    a = np.array(arr)\n",
        "    return f\"{a.mean():.3f} ± {a.std():.3f}\"\n",
        "\n",
        "print(\"\\n=== DNDF Test Results (n=100 bootstraps) ===\")\n",
        "print(f\"AUC-ROC  : {fmt(metrics['roc_auc'])}\")\n",
        "print(f\"AUC-PR   : {fmt(metrics['pr_auc'])}\")\n",
        "for name in cutoffs:\n",
        "    print(f\"{name:12} Precision: {fmt(metrics[f'{name}_P'])}\")\n",
        "    print(f\"{name:12} Recall   : {fmt(metrics[f'{name}_R'])}\")\n",
        "    print(f\"{name:12} F1-score : {fmt(metrics[f'{name}_F1'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErNUk6Bl96Ps",
        "outputId": "749b8ff3-fb46-43f9-aee8-11cce0cfa628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DNDF Test Results (n=100 bootstraps) ===\n",
            "AUC-ROC  : 0.759 ± 0.013\n",
            "AUC-PR   : 0.584 ± 0.020\n",
            "Top 0.1%     Precision: 1.000 ± 0.000\n",
            "Top 0.1%     Recall   : 0.035 ± 0.002\n",
            "Top 0.1%     F1-score : 0.067 ± 0.003\n",
            "Top 1%       Precision: 0.995 ± 0.005\n",
            "Top 1%       Recall   : 0.378 ± 0.018\n",
            "Top 1%       F1-score : 0.548 ± 0.019\n",
            "Top 10%      Precision: 0.159 ± 0.009\n",
            "Top 10%      Recall   : 0.607 ± 0.021\n",
            "Top 10%      F1-score : 0.252 ± 0.013\n",
            "Prevalence   Precision: 0.290 ± 0.017\n",
            "Prevalence   Recall   : 0.582 ± 0.021\n",
            "Prevalence   F1-score : 0.387 ± 0.019\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}