{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baicheto/AML_Bitcoin/blob/Kri/AML_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepWalk"
      ],
      "metadata": {
        "id": "pBgFUZ7NwGWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It's needed to run the section `I. Centralities` only once.**"
      ],
      "metadata": {
        "id": "pnvKUgdnqXxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Centralities"
      ],
      "metadata": {
        "id": "uoWl3O4Zmrfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CENT_FILES = {\n",
        "    'degree'     : 'degree_centrality.pkl',\n",
        "    'betweenness': 'betweenness_centrality.pkl',\n",
        "    'closeness'  : 'closeness_centrality.pkl',\n",
        "    'eigenvector': 'eigen_centrality.pkl',\n",
        "}\n",
        "\n",
        "centralities = {}"
      ],
      "metadata": {
        "id": "6nUEX7dZvNdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = CENT_FILES['degree']\n",
        "if os.path.exists(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        deg = pickle.load(f)\n",
        "else:\n",
        "    deg = nx.degree_centrality(G_int_sub)\n",
        "\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(deg, f)\n",
        "    print(f\"Saved PKL {fname}\")\n",
        "\n",
        "inv_map = { i: tx for tx, i in tx_idx_sub.items() }\n",
        "\n",
        "ser_deg = pd.Series(\n",
        "    { inv_map[i]: deg[i] for i in deg },\n",
        "    name='degree'\n",
        ")\n",
        "\n",
        "df_deg = ser_deg.reindex(all_nodes_sub).fillna(0).to_frame()\n",
        "df_deg.index.name = 'txId'\n",
        "\n",
        "csv_file = 'degree_centrality.csv'\n",
        "df_deg.to_csv(csv_file)\n",
        "print(f\"Saved CSV {csv_file}\")\n",
        "files.download(csv_file)\n",
        "\n",
        "centralities['degree'] = df_deg['degree'].to_dict()"
      ],
      "metadata": {
        "id": "foOEjoCKu_qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = CENT_FILES['betweenness']\n",
        "if os.path.exists(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        btw = pickle.load(f)\n",
        "else:\n",
        "    btw = nx.betweenness_centrality(G_int_sub)\n",
        "\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(btw, f)\n",
        "    print(f\"Saved PKL {fname}\")\n",
        "\n",
        "inv_map = { i: tx for tx, i in tx_idx_sub.items() }\n",
        "\n",
        "ser_btw = pd.Series(\n",
        "    { inv_map[i]: btw[i] for i in btw },\n",
        "    name='betweenness'\n",
        ")\n",
        "\n",
        "df_btw = ser_btw.reindex(all_nodes_sub).fillna(0).to_frame()\n",
        "df_btw.index.name = 'txId'\n",
        "\n",
        "csv_file = 'betweenness_centrality.csv'\n",
        "df_btw.to_csv(csv_file)\n",
        "print(f\"Saved CSV {csv_file}\")\n",
        "files.download(csv_file)\n",
        "\n",
        "centralities['betweenness'] = df_btw['betweenness'].to_dict()"
      ],
      "metadata": {
        "id": "COXVDBb7wIQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = CENT_FILES['closeness']\n",
        "if os.path.exists(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        clo = pickle.load(f)\n",
        "else:\n",
        "    clo = nx.closeness_centrality(G_int_sub)\n",
        "\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(clo, f)\n",
        "    print(f\"Saved PKL {fname}\")\n",
        "\n",
        "inv_map = { i: tx for tx, i in tx_idx_sub.items() }\n",
        "\n",
        "ser_clo = pd.Series(\n",
        "    { inv_map[i]: clo[i] for i in clo },\n",
        "    name='closeness'\n",
        ")\n",
        "\n",
        "df_clo = ser_clo.reindex(all_nodes_sub).fillna(0).to_frame()\n",
        "df_clo.index.name = 'txId'\n",
        "\n",
        "csv_file = 'closeness_centrality.csv'\n",
        "df_clo.to_csv(csv_file)\n",
        "print(f\"Saved CSV {csv_file}\")\n",
        "files.download(csv_file)\n",
        "\n",
        "centralities['closeness'] = df_clo['closeness'].to_dict()"
      ],
      "metadata": {
        "id": "VUmhi1VOx6ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = CENT_FILES['eigenvector']\n",
        "if os.path.exists(fname):\n",
        "    with open(fname, 'rb') as f:\n",
        "        deg = pickle.load(f)\n",
        "else:\n",
        "    eig = nx.eigenvector_centrality(G_int_sub)\n",
        "\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(eig, f)\n",
        "    print(f\"Saved PKL {fname}\")\n",
        "\n",
        "inv_map = { i: tx for tx, i in tx_idx_sub.items() }\n",
        "\n",
        "ser_eig = pd.Series(\n",
        "    { inv_map[i]: eig[i] for i in eig },\n",
        "    name='eigenvector'\n",
        ")\n",
        "\n",
        "df_eig = ser_eig.reindex(all_nodes_sub).fillna(0).to_frame()\n",
        "df_eig.index.name = 'txId'\n",
        "\n",
        "csv_file = 'eigenvector_centrality.csv'\n",
        "df_eig.to_csv(csv_file)\n",
        "print(f\"Saved CSV {csv_file}\")\n",
        "files.download(csv_file)\n",
        "\n",
        "centralities['eigenvector'] = df_eig['eigenvector'].to_dict()"
      ],
      "metadata": {
        "id": "ZpudsSjgy9UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II.  Manual Features"
      ],
      "metadata": {
        "id": "UQH2XTw2hUod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('degree_centrality.pkl','rb')      as f: degree     = pickle.load(f)\n",
        "with open('betweenness_centrality.pkl','rb') as f: betweenness= pickle.load(f)\n",
        "with open('closeness_centrality.pkl','rb')   as f: closeness  = pickle.load(f)\n",
        "with open('eigenvector_centrality.pkl','rb') as f: eigen      = pickle.load(f)"
      ],
      "metadata": {
        "id": "wu5Ml8PEiEWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cent = pd.DataFrame({\n",
        "    'degree'     : pd.Series(degree),\n",
        "    'betweenness': pd.Series(betweenness),\n",
        "    'closeness'  : pd.Series(closeness),\n",
        "    'eigenvector': pd.Series(eigen),\n",
        "})\n",
        "df_cent = df_cent.reindex(all_nodes_sub).fillna(0)\n",
        "df_cent.index.name = 'txId'"
      ],
      "metadata": {
        "id": "nDsDjV2ShJ9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_cent = torch.tensor(df_cent.values, dtype=torch.float, device=device)"
      ],
      "metadata": {
        "id": "lK86JaWhifQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_tensor = torch.cat([feat_intr, feat_cent], dim=1)"
      ],
      "metadata": {
        "id": "yxdVyaIAih_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_sub = len(all_nodes_sub)\n",
        "feat_tensor.shape == (N_sub, len(trans_features) + 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTVjhkrojfZm",
        "outputId": "75f1dbf3-c213-494a-b599-c569f05b857d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_known = labels >= 0\n",
        "idx_known  = mask_known.nonzero(as_tuple=False).view(-1)\n",
        "ts_known   = ts_tensor[mask_known]\n",
        "\n",
        "train_idx = idx_known[ ts_known <= 30 ]\n",
        "val_idx   = idx_known[(ts_known >= 31) & (ts_known <= 40)]\n",
        "test_idx  = idx_known[ ts_known >= 41 ]"
      ],
      "metadata": {
        "id": "YCuu5vPZor5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepWalk Machinery"
      ],
      "metadata": {
        "id": "MuPBOGsPym-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_walk(graph, start, walk_length, rng=None):\n",
        "    walk = [start]\n",
        "    if rng is None:\n",
        "        rng = random.Random()\n",
        "    for _ in range(walk_length - 1):\n",
        "        cur = walk[-1]\n",
        "        nbrs = list(graph[cur])\n",
        "        if not nbrs:\n",
        "            break\n",
        "        walk.append(rng.choice(nbrs))\n",
        "    return walk"
      ],
      "metadata": {
        "id": "lVz8EolI5QTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _walk_worker(args):\n",
        "    graph, batch, wl, wpn = args\n",
        "    rng = random.Random()\n",
        "    walks = []\n",
        "    for n in batch:\n",
        "        for _ in range(wpn):\n",
        "            walks.append(random_walk(graph, n, wl, rng))\n",
        "    return walks"
      ],
      "metadata": {
        "id": "fEDd7713zGiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_corpus(graph, walk_length=3, walks_per_node=2, workers=None):\n",
        "    if workers is None:\n",
        "        workers = mp.cpu_count()\n",
        "    nodes   = list(graph.nodes())\n",
        "    np.random.shuffle(nodes)\n",
        "    batches = np.array_split(nodes, workers)\n",
        "    with mp.Pool(workers) as pool:\n",
        "        results = pool.map(\n",
        "            _walk_worker,\n",
        "            [(graph, b, walk_length, walks_per_node) for b in batches]\n",
        "        )\n",
        "    return [w for sub in results for w in sub]"
      ],
      "metadata": {
        "id": "sLg1v4Bxzs6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(\n",
        "    sentences,\n",
        "    dim=5,\n",
        "    window=2,\n",
        "    epochs=176,\n",
        "    neg=1\n",
        "):\n",
        "    return Word2Vec(\n",
        "        sentences=sentences,\n",
        "        vector_size=dim,\n",
        "        window=window,\n",
        "        min_count=0,\n",
        "        sg=1,\n",
        "        hs=0,\n",
        "        negative=neg,\n",
        "        workers=1,\n",
        "        epochs=epochs,\n",
        "        seed=42\n",
        "    )"
      ],
      "metadata": {
        "id": "yDxz5qqG0OB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings(w2v, path_pt='dwS.pt', path_txt='dwS.txt'):\n",
        "    vecs = torch.tensor(w2v.wv.vectors)\n",
        "    torch.save(vecs, path_pt)\n",
        "    with open(path_txt, 'w') as f:\n",
        "        N, D = vecs.shape\n",
        "        f.write(f\"{N} {D}\\n\")\n",
        "        for node in w2v.wv.key_to_index:\n",
        "            coords = \" \".join(map(str, w2v.wv[node]))\n",
        "            f.write(f\"{node} {coords}\\n\")"
      ],
      "metadata": {
        "id": "EL-E1J370bxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_deepwalk(graph, out_pt='dwS.pt', out_txt='dwS.txt'):\n",
        "    params = {\n",
        "        'walk_length':  3,\n",
        "        'walks_per_node': 2,\n",
        "        'dim':          5,\n",
        "        'window':       2,\n",
        "        'neg':          1,\n",
        "        'epochs':      176,\n",
        "        'out_pt':      out_pt,\n",
        "        'out_txt':     out_txt\n",
        "    }\n",
        "    corpus = build_corpus(\n",
        "        graph,\n",
        "        walk_length=params['walk_length'],\n",
        "        walks_per_node=params['walks_per_node']\n",
        "    )\n",
        "    w2v = train_word2vec(\n",
        "        corpus,\n",
        "        dim=params['dim'],\n",
        "        window=params['window'],\n",
        "        epochs=params['epochs'],\n",
        "        neg=params['neg']\n",
        "    )\n",
        "    save_embeddings(w2v, params['out_pt'], params['out_txt'])\n",
        "    return params['out_pt']"
      ],
      "metadata": {
        "id": "1qBsKfjG0qps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare training‐only subgraph, then full‐graph embeddings"
      ],
      "metadata": {
        "id": "XGryHFx61UYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tv_nodes = torch.cat([train_idx, val_idx]).cpu().numpy().tolist()\n",
        "G_tv     = G_int_sub.subgraph(tv_nodes).copy()\n",
        "\n",
        "_ = run_deepwalk(G_tv, out_pt='dwS_tv.pt', out_txt='ignore.txt')"
      ],
      "metadata": {
        "id": "_0itKHZk1jL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_pt = run_deepwalk(G_int_sub, out_pt='dwS_full.pt', out_txt='ignore.txt')\n",
        "Z_full = torch.load(emb_pt).to(device)"
      ],
      "metadata": {
        "id": "mXricTgg1qgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downstream “decoder” (2‐hidden‐layer NN)"
      ],
      "metadata": {
        "id": "el4o5lRl1z7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepWalkDecoder(nn.Module):\n",
        "    def __init__(self, Z, feat, hidden=10):\n",
        "        super().__init__()\n",
        "        N, D = Z.shape\n",
        "        F    = feat.size(1)\n",
        "        self.emb  = nn.Embedding.from_pretrained(Z, freeze=False)\n",
        "        self.fc1  = nn.Linear(D + F, hidden)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2  = nn.Linear(hidden, hidden)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.out  = nn.Linear(hidden, 2)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        x_emb  = self.emb(idx)\n",
        "        x_feat = feat_tensor[idx]\n",
        "        x      = torch.cat([x_emb, x_feat], dim=1)\n",
        "        x      = self.drop(self.act1(self.fc1(x)))\n",
        "        x      = self.drop(self.act2(self.fc2(x)))\n",
        "        return self.out(x)"
      ],
      "metadata": {
        "id": "wqu1px4j16Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = DeepWalkDecoder(Z_full, feat_tensor).to(device)"
      ],
      "metadata": {
        "id": "vCMI0L0t19QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt  = optim.Adam(decoder.parameters(), lr=0.0554)\n",
        "crit = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ZsE8TCoR1_O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val, wait = -1, 0\n",
        "for epoch in range(1, 81):\n",
        "    decoder.train()\n",
        "    opt.zero_grad()\n",
        "    logits = decoder(train_idx)\n",
        "    loss   = crit(logits, labels[train_idx])\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = decoder(val_idx)\n",
        "        val_probs  = torch.softmax(val_logits, dim=1)[:,1].cpu().numpy()\n",
        "        val_pr     = average_precision_score(labels[val_idx].cpu(), val_probs)\n",
        "\n",
        "    if val_pr > best_val:\n",
        "        best_val, wait = val_pr, 0\n",
        "        torch.save(decoder.state_dict(), \"dwS_decoder_best.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= 5:\n",
        "            break"
      ],
      "metadata": {
        "id": "8x-nXOXv3aEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final test‐set evaluation"
      ],
      "metadata": {
        "id": "ZFdbS9GX4AQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "    test_logits = decoder(test_idx)\n",
        "    test_probs  = torch.softmax(test_logits, dim=1)[:,1].cpu().numpy()\n",
        "\n",
        "M   = len(test_probs)\n",
        "k   = max(1, int(0.01 * M))\n",
        "th  = np.sort(test_probs)[-k]\n",
        "top = (test_probs >= th).astype(int)"
      ],
      "metadata": {
        "id": "kaWeClRDjlHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "    test_logits = decoder(test_idx)\n",
        "    test_probs  = torch.softmax(test_logits, dim=1)[:,1].cpu().numpy()\n",
        "    test_true   = labels[test_idx].cpu().numpy()\n",
        "\n",
        "M_test = len(test_probs)\n",
        "\n",
        "train_true = labels[train_idx].cpu().numpy()\n",
        "prevalence = train_true.mean()\n",
        "k_prev     = max(1, int(prevalence * M_test))\n",
        "\n",
        "cutoffs = {\n",
        "    \"Top 0.1%\" : max(1, int(0.001 * M_test)),\n",
        "    \"Top 1%\"   : max(1, int(0.01  * M_test)),\n",
        "    \"Top 10%\"  : max(1, int(0.10  * M_test)),\n",
        "    \"Prevalence\" : k_prev\n",
        "}"
      ],
      "metadata": {
        "id": "q1bUu5wF_i6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_runs = 100\n",
        "metrics = {\n",
        "    \"roc_auc\":     [],\n",
        "    \"pr_auc\":      [],\n",
        "    **{f\"{name}_P\": [] for name in cutoffs},\n",
        "    **{f\"{name}_R\": [] for name in cutoffs},\n",
        "    **{f\"{name}_F1\": [] for name in cutoffs},\n",
        "}\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "for _ in range(n_runs):\n",
        "    idxs = rng.choice(M_test, size=M_test, replace=True)\n",
        "    y_true  = test_true[idxs]\n",
        "    y_score = test_probs[idxs]\n",
        "\n",
        "    metrics[\"roc_auc\"].append(roc_auc_score(y_true, y_score))\n",
        "    metrics[\"pr_auc\"].append(average_precision_score(y_true, y_score))\n",
        "\n",
        "    sorted_idx = np.argsort(y_score)\n",
        "    for name, k in cutoffs.items():\n",
        "        topk = sorted_idx[-k:]\n",
        "        y_pred = np.zeros_like(y_score, dtype=int)\n",
        "        y_pred[topk] = 1\n",
        "\n",
        "        metrics[f\"{name}_P\"].append(precision_score(y_true, y_pred))\n",
        "        metrics[f\"{name}_R\"].append(recall_score(y_true, y_pred))\n",
        "        metrics[f\"{name}_F1\"].append(f1_score(y_true, y_pred))"
      ],
      "metadata": {
        "id": "pphfUxM0_yku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fmt(name):\n",
        "    vals = np.array(metrics[name])\n",
        "    return f\"{vals.mean():.3f} ± {vals.std():.3f}\"\n",
        "\n",
        "print(\"=== Bootstrap Test‐Set Results (n=100) ===\")\n",
        "print(f\"ROC AUC : {fmt('roc_auc')}\")\n",
        "print(f\"PR  AUC : {fmt('pr_auc')}\")\n",
        "for name in cutoffs:\n",
        "    print(f\"{name:12} Precision  : {fmt(name+'_P')}\")\n",
        "    print(f\"{name:12} Recall     : {fmt(name+'_R')}\")\n",
        "    print(f\"{name:12} F1-Score   : {fmt(name+'_F1')}\")"
      ],
      "metadata": {
        "id": "cXpKcQeA_6qm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6386439-0e8f-4334-d95c-7755db9213a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bootstrap Test‐Set Results (n=100) ===\n",
            "ROC AUC : 0.796 ± 0.013\n",
            "PR  AUC : 0.503 ± 0.024\n",
            "Top 0.1%     Precision  : 1.000 ± 0.000\n",
            "Top 0.1%     Recall     : 0.017 ± 0.001\n",
            "Top 0.1%     F1-Score   : 0.034 ± 0.002\n",
            "Top 1%       Precision  : 0.943 ± 0.038\n",
            "Top 1%       Recall     : 0.179 ± 0.009\n",
            "Top 1%       F1-Score   : 0.300 ± 0.014\n",
            "Top 10%      Precision  : 0.298 ± 0.018\n",
            "Top 10%      Recall     : 0.567 ± 0.023\n",
            "Top 10%      F1-Score   : 0.390 ± 0.020\n",
            "Prevalence   Precision  : 0.274 ± 0.017\n",
            "Prevalence   Recall     : 0.573 ± 0.023\n",
            "Prevalence   F1-Score   : 0.371 ± 0.019\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wT_Hq7pQVWh5",
        "nVccEhwt8dqJ",
        "ikS1ibqmaA91",
        "DDp67i4xRpEq",
        "uoWl3O4Zmrfd",
        "MuPBOGsPym-N",
        "XGryHFx61UYk",
        "el4o5lRl1z7K",
        "ZFdbS9GX4AQM",
        "v5Deh8OgQuMK",
        "HFjReNLQ4ZcK",
        "VTprORl2q3cO",
        "15gw4TIUqyBc",
        "TXDM8dmasrKl",
        "f2273qCTs5hJ",
        "PVCEARc7gLPZ",
        "DOW5Pnzi27K4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}